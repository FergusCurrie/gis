{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "REPO_DIR = '/home/fergus/repos/dinov3'\n",
    "WEIGHTS_PATH = '/home/fergus/dinov2_vitg14_reg4_pretrain.pth'\n",
    "WEIGHTS_PATH = '/home/fergus/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth'\n",
    "\n",
    "\n",
    "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "SAT_SMALL = 'dinov3_vitl16'\n",
    "SAT_BIG = 'dinov3_vit7b16'\n",
    "\n",
    "\n",
    "\n",
    "dinov3_vitl16 = torch.hub.load(REPO_DIR, SAT_SMALL, source='local', weights=WEIGHTS_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in dinov3_vitl16.named_parameters():\n",
    "    param.requires_grad = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov3_vitl16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Usage\n",
    "total, trainable = count_parameters(dinov3_vitl16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "= https://www.labellerr.com/blog/dinov3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_model_forward(model, x):\n",
    "    print(\"=== Model Forward Trace ===\")\n",
    "    \n",
    "    # Step 1: patch embedding\n",
    "    patches = model.patch_embed(x)\n",
    "    print(f\"1. patch_embed output: {patches.shape}\")\n",
    "    print(f\"   First few values: {patches[0, 0, :5]}\")\n",
    "    \n",
    "    # Step 2: through blocks manually\n",
    "    x_manual = patches.clone()\n",
    "    for i, block in enumerate(model.blocks):\n",
    "        x_manual = block(x_manual)\n",
    "        if i == 0:\n",
    "            print(f\"2. After first block: {x_manual.shape}\")\n",
    "            print(f\"   First few values: {x_manual[0, 0, :5]}\")\n",
    "    \n",
    "    # Step 3: final norm\n",
    "    x_manual = model.norm(x_manual)\n",
    "    print(f\"3. After final norm: {x_manual.shape}\")\n",
    "    print(f\"   First few values: {x_manual[0, 0, :5]}\")\n",
    "    \n",
    "    # Step 4: full model call\n",
    "    full_output = model(x)\n",
    "    print(f\"4. Full model() output: {full_output.shape}\")\n",
    "    print(f\"   First few values: {full_output[0, :5]}\")\n",
    "    \n",
    "    # Compare patch_embed vs final processed patches\n",
    "    if x_manual.shape[1] > 1:  # If we have patch tokens\n",
    "        comparison_tensor = x_manual[:, 1:]  # Skip CLS if present\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "            patches.flatten(1), \n",
    "            comparison_tensor.flatten(1), \n",
    "            dim=1\n",
    "        )\n",
    "        print(f\"5. Similarity between patch_embed and processed patches: {cosine_sim.item():.4f}\")\n",
    "    \n",
    "    return patches, x_manual, full_output\n",
    "\n",
    "# Run the trace\n",
    "patch_embed_out, final_patches, model_out = trace_model_forward(dinov3_vitl16, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dinov3_vitl16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov3_vitl16.patch_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = dinov3_vitl16.forward_features(inp)\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff['x_norm_clstoken'].shape, ff['x_storage_tokens'].shape, ff['x_norm_patchtokens'].shape, ff['x_prenorm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov3_vitl16.rope_embed(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_transform(resize_size: int = 224):\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    resize = transforms.Resize((resize_size, resize_size), antialias=True)\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=(0.430, 0.411, 0.296),\n",
    "        std=(0.213, 0.156, 0.143),\n",
    "    )\n",
    "    return transforms.Compose([to_tensor, resize, normalize])\n",
    "\n",
    "def make_transform_mask(resize_size: int = 224):\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    resize = transforms.Resize((resize_size, resize_size), antialias=True)\n",
    "    return transforms.Compose([to_tensor, resize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import cv2 \n",
    "\n",
    "files = os.listdir(f'/mnt/gis/image/18') \n",
    "img = cv2.imread(f'/mnt/gis/image/18/{files[1]}')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(f'/mnt/gis/image/18') \n",
    "img = cv2.imread(f'/mnt/gis/image/18/{files[9]}')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "transform = make_transform()\n",
    "inp = transform(img)\n",
    "inp.shape\n",
    "inp = torch.unsqueeze(inp, 0)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dinov3_vitl16(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = dinov3_vitl16.patch_embed(inp)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_tokens_only(model, x):\n",
    "    # Run full forward pass but return all tokens\n",
    "    with torch.no_grad():\n",
    "        # Manually run through the model\n",
    "        x = model.patch_embed(x)\n",
    "        \n",
    "        # Through transformer blocks\n",
    "        for block in model.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        # Final normalization\n",
    "        x = model.norm(x)\n",
    "        \n",
    "        # Split CLS and patch tokens\n",
    "        # Assuming first token is CLS (this might vary by model)\n",
    "        if x.shape[1] > 1:  # More than just CLS token\n",
    "            patch_tokens = x[:, 1:]  # All except first token\n",
    "            cls_token = x[:, 0:1]    # First token only\n",
    "        else:\n",
    "            # If only CLS token, there are no patch tokens\n",
    "            patch_tokens = None\n",
    "            cls_token = x\n",
    "            \n",
    "        return patch_tokens, cls_token\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "patch_size = 14 *2\n",
    "\n",
    "feature_dim = 1024\n",
    "\n",
    "features = emb[0].detach().numpy()\n",
    "\n",
    "\n",
    "patches = features.reshape(-1, feature_dim)\n",
    "print(patches.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "patches_scaled = scaler.fit_transform(patches)\n",
    "\n",
    "# Apply PCA to reduce to 3 components (RGB)\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "pca_result = pca.fit_transform(patches_scaled) \n",
    "pca_result.shape\n",
    "\n",
    "pca_min = pca_result.min(axis=0)\n",
    "pca_max = pca_result.max(axis=0)\n",
    "pca_normalized = (pca_result - pca_min) / (pca_max - pca_min + 1e-8)\n",
    "\n",
    "\n",
    "pca_spatial = pca_normalized.reshape(patch_size, patch_size, 3)\n",
    "pca_spatial.shape\n",
    "\n",
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "axs[0].imshow(img)\n",
    "axs[1].imshow(img)\n",
    "axs[2].imshow(pca_spatial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from gis.config import Config\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "config = Config()\n",
    "\n",
    "def get_image_and_mask_files():\n",
    "    mask_files = os.listdir(config.mnt_path / 'label/18')\n",
    "    coords = []\n",
    "    for mask in mask_files:\n",
    "        x,y = mask.split('_')\n",
    "        x,y = int(x), int(y.replace('.npy', ''))\n",
    "        coords.append((x,y))\n",
    "    image_files = [f'18_{x}_{y}.jpg' for (x,y) in coords]\n",
    "    return image_files, mask_files\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks \n",
    "        self.transform = transform\n",
    "        self.image_dir = config.mnt_path / 'image/18'\n",
    "        self.mask_dir = config.mnt_path / 'label/18'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])  \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_image = np.array(image)\n",
    "        original_mask = np.load(mask_path)\n",
    "\n",
    "        #transformed = self.transform(image=original_image, mask=original_mask)\n",
    "        image = self.transform(original_image)\n",
    "        #print(original_image.shape)\n",
    "        image, target = torch.tensor(image), torch.LongTensor(original_mask)\n",
    "        #print(image.shape)\n",
    "        #mage = image.permute(2,0,1)\n",
    "        #print(image.shape)\n",
    "        return image, target, original_image, original_mask\n",
    "    \n",
    "model_config = {\n",
    "        'batch_size': 4,\n",
    "        'epochs': 5,\n",
    "        'learning_rate': 1e-4,\n",
    "        'val_split': 0.2,\n",
    "        'num_workers': 4,\n",
    "    }\n",
    "\n",
    "\n",
    "image_files, mask_files = get_image_and_mask_files()\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    image_files, mask_files, test_size=model_config['val_split'], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_transform = make_transform()\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, transform=train_transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, transform=make_transform_mask())\n",
    "\n",
    "\n",
    "train_loader =  DataLoader(train_dataset, batch_size=model_config['batch_size'])\n",
    "val_loader =  DataLoader(val_dataset, batch_size=model_config['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "    \n",
    "\"\"\"\n",
    "It basically is. In the paper they state:\n",
    "\n",
    "We perform linear probing on top of the dense features for two tasks: semantic segmentation and monocular depth estimation. \n",
    "In both cases, we train a linear transform on top of the frozen patch outputs of DINOv3.\n",
    "\n",
    "Meaning they use the frozen DINOv3, train a linear layer nn.Linear(768, 256) \n",
    "and then you simply have to reshape the 256 to your 16x16 patch. \n",
    "Afterwards you would have a 224x224 segmentation image if the input is also 224x224.\n",
    "\n",
    "https://www.reddit.com/r/computervision/comments/1mrvhrp/not_understanding_the_dense_feature_maps_of_dinov3/\n",
    "\n",
    "[1, 784, 1024]\n",
    "B, P*P, F\n",
    "\"\"\"\n",
    "\n",
    "class DINOv3Seg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DINOv3Seg, self).__init__()\n",
    "\n",
    "        self.dino = torch.hub.load(REPO_DIR, SAT_SMALL, source='local', weights=WEIGHTS_PATH)\n",
    "\n",
    "        for param in self.dino.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.head = nn.Linear(384, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.dino.forward_features(x)['x_norm_patchtokens'] # torch.Size([1, 784, 1024])\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Unet inte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:\n",
    "            diffY = x1.size()[2] - x2.size()[2]\n",
    "            diffX = x1.size()[3] - x2.size()[3]\n",
    "            x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "            x = torch.cat([x1, x2], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        x = self.up(x)\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class DINOv3_UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DINOv3_UNet, self).__init__()\n",
    "\n",
    "        self.dino = torch.hub.load(REPO_DIR, SAT_SMALL, source='local', weights=WEIGHTS_PATH)\n",
    "\n",
    "        for param in self.dino.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.reduce1 = nn.Conv2d(1024, 128, 1)\n",
    "        self.reduce2 = nn.Conv2d(1024, 128, 1)\n",
    "        self.reduce3 = nn.Conv2d(1024, 128, 1)\n",
    "        self.reduce4 = nn.Conv2d(1024, 128, 1)\n",
    "\n",
    "        self.up1 = Up(256, 128)\n",
    "        self.up2 = Up(256, 128)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 128)\n",
    "        self.head = nn.Conv2d(128, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.dino.forward_features(x)['x_norm_patchtokens']\n",
    "        x = x.view(B, H//16, W//16, -1).permute(0, 3, 1, 2)\n",
    "        # Create 4 different scales from same features \n",
    "        x1 = F.interpolate(self.reduce1(x), size=(H//4, W//4), mode='bilinear')\n",
    "        x2 = F.interpolate(self.reduce2(x), size=(H//8, W//8), mode='bilinear')\n",
    "        x3 = F.interpolate(self.reduce3(x), size=(H//16, W//16), mode='bilinear')\n",
    "        x4 = F.interpolate(self.reduce4(x), size=(H//32, W//32), mode='bilinear')\n",
    "        x = self.up4(x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up1(x, x1)\n",
    "        out = self.head(x)\n",
    "        out = F.interpolate(self.head(x), scale_factor=2, mode='bilinear')\n",
    "        return out\n",
    "\n",
    "# REPO_DIR = '/home/fergus/repos/dinov3'\n",
    "# WEIGHTS_PATH = '/home/fergus/dinov2_vitg14_reg4_pretrain.pth'\n",
    "# WEIGHTS_PATH = '/home/fergus/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth'\n",
    "# dinoUnet = DINOv3_UNet()\n",
    "# count_parameters(dinoUnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleConv(nn.Module):\n",
    "    \"\"\"Single convolution => [BN] => ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class LightweightUp(nn.Module):\n",
    "    \"\"\"Lightweight upscaling with single conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = SingleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:\n",
    "            # Pad if needed\n",
    "            diffY = x1.size()[2] - x2.size()[2]\n",
    "            diffX = x1.size()[3] - x2.size()[3]\n",
    "            x2 = F.pad(x2, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "            x = torch.cat([x1, x2], dim=1)\n",
    "        else:\n",
    "            x = x1\n",
    "        x = self.up(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UltraLightDINOv3_UNet(nn.Module):\n",
    "    \"\"\"Ultra-lightweight version with ~300K parameters\"\"\"\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load DINOv3 (frozen)\n",
    "        self.dino = torch.hub.load(REPO_DIR, SAT_SMALL, source='local', weights=WEIGHTS_PATH)\n",
    "        for param in self.dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Much smaller channel dimensions\n",
    "        hidden_dim = 32  # Reduced from 128 to 32\n",
    "        \n",
    "        # Channel reduction layers\n",
    "        self.reduce1 = nn.Conv2d(1024, hidden_dim, 1)\n",
    "        self.reduce2 = nn.Conv2d(1024, hidden_dim, 1)\n",
    "        self.reduce3 = nn.Conv2d(1024, hidden_dim, 1)\n",
    "        self.reduce4 = nn.Conv2d(1024, hidden_dim, 1)\n",
    "        \n",
    "        # Lightweight decoder\n",
    "        self.up1 = LightweightUp(hidden_dim * 2, hidden_dim)\n",
    "        self.up2 = LightweightUp(hidden_dim * 2, hidden_dim)\n",
    "        self.up3 = LightweightUp(hidden_dim * 2, hidden_dim)\n",
    "        self.up4 = LightweightUp(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Final head\n",
    "        self.head = nn.Conv2d(hidden_dim, num_classes, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Extract DINOv3 features\n",
    "        features = self.dino.forward_features(x)['x_norm_patchtokens']\n",
    "        features = features.view(B, H//16, W//16, -1).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Create multi-scale features\n",
    "        x1 = F.interpolate(self.reduce1(features), size=(H//4, W//4), mode='bilinear')\n",
    "        x2 = F.interpolate(self.reduce2(features), size=(H//8, W//8), mode='bilinear')\n",
    "        x3 = F.interpolate(self.reduce3(features), size=(H//16, W//16), mode='bilinear')\n",
    "        x4 = F.interpolate(self.reduce4(features), size=(H//32, W//32), mode='bilinear')\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up4(x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up1(x, x1)\n",
    "        \n",
    "        # Final output\n",
    "        out = self.head(x)\n",
    "        out = F.interpolate(out, scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Even more extreme version (~100K parameters)\n",
    "class MinimalDINOv3_UNet(nn.Module):\n",
    "    \"\"\"Minimal version with ~100K parameters\"\"\"\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load DINOv3 (frozen)\n",
    "        self.dino = torch.hub.load(REPO_DIR, SAT_SMALL, source='local', weights=WEIGHTS_PATH)\n",
    "        for param in self.dino.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Very small channels\n",
    "        hidden_dim = 16\n",
    "        \n",
    "        # Single channel reduction\n",
    "        self.reduce = nn.Conv2d(1024, hidden_dim, 1)\n",
    "        \n",
    "        # Simple decoder without skip connections\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, num_classes, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.dino.forward_features(x)['x_norm_patchtokens']\n",
    "        features = features.view(B, H//16, W//16, -1).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Reduce channels and decode\n",
    "        x = self.reduce(features)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        # Upsample to final size\n",
    "        out = F.interpolate(x, size=(H//2, W//2), mode='bilinear')\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "model_minimal = MinimalDINOv3_UNet(num_classes=1)\n",
    "print(\"\\n=== Minimal Model ===\")\n",
    "count_parameters(model_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate IoU, Dice, and other metrics.\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    # IoU\n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    union = pred_binary.sum() + target_binary.sum() - intersection\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    \n",
    "    # Dice coefficient\n",
    "    dice = (2 * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-8)\n",
    "    \n",
    "    # Pixel accuracy\n",
    "    correct = (pred_binary == target_binary).sum()\n",
    "    total = target_binary.numel()\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return {\n",
    "        'iou': iou.item(),\n",
    "        'dice': dice.item(),\n",
    "        'accuracy': accuracy.item()\n",
    "    }\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_metrics = {'iou': 0.0, 'dice': 0.0, 'accuracy': 0.0}\n",
    "    \n",
    "    for images, masks, _, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        print(images.shape, masks.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        print(outputs.shape, masks.shape, masks.unsqueeze(1).shape)\n",
    "        \n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            batch_metrics = calculate_metrics(torch.sigmoid(outputs), masks.unsqueeze(1))\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key] += batch_metrics[key]\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    for key in train_metrics:\n",
    "        train_metrics[key] /= len(train_loader)\n",
    "    \n",
    "    return train_loss, train_metrics\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'iou': 0.0, 'dice': 0.0, 'accuracy': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks, _, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1))  # Add channel dim for masks\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_metrics = calculate_metrics(outputs, masks.unsqueeze(1))\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] += batch_metrics[key]\n",
    "    \n",
    "    # Average metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    for key in val_metrics:\n",
    "        val_metrics[key] /= len(val_loader)\n",
    "    \n",
    "    return val_loss, val_metrics\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.AdamW(model_minimal.parameters(), lr=model_config['learning_rate'])\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "model = model_minimal.to(device)\n",
    "\n",
    "\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_iou': [], 'val_iou': [],\n",
    "    'train_dice': [], 'val_dice': [],\n",
    "    'train_accuracy': [], 'val_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model, train_loader, loss_fn, optimizer, device\n",
    "    )\n",
    "    val_loss, val_metrics = validate_model(model, val_loader, loss_fn, device) # todo: get eval loaders \n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_iou'].append(train_metrics['iou'])\n",
    "    history['val_iou'].append(val_metrics['iou'])\n",
    "    history['train_dice'].append(train_metrics['dice'])\n",
    "    history['val_dice'].append(val_metrics['dice'])\n",
    "    history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train IoU: {train_metrics['iou']:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val IoU: {val_metrics['iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
