{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "preprocess_input = get_preprocessing_fn(\"timm-efficientnet-b3\", pretrained='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.path.exists('/mnt/gis/image/18/18_231006_155459.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, image_dir, mask_dir, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks \n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        # print(self.image_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])  \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=np.array(mask))\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        else:\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        # image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        # mask = torch.from_numpy(mask).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "img_dir = '/mnt/gis/image/18/'\n",
    "mask_dir = '/mnt/gis/label/18/'\n",
    "images = os.listdir(img_dir)\n",
    "masks = os.listdir(mask_dir)\n",
    "\n",
    "\n",
    "coords = []\n",
    "for mask in masks:\n",
    "    x,y = mask.split('_')\n",
    "    x,y = int(x), int(y.replace('.npy', ''))\n",
    "    coords.append((x,y))\n",
    "\n",
    "images = [f'18_{x}_{y}.jpg' for (x,y) in coords]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "        'batch_size': 4,\n",
    "        'epochs': 5,\n",
    "        'learning_rate': 1e-4,\n",
    "        'val_split': 0.2,\n",
    "        'num_workers': 4,\n",
    "    }\n",
    "\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=config['val_split'], random_state=42\n",
    ")\n",
    "\n",
    "TARGET_SIZE = (256, 256) \n",
    "\n",
    "transform = A.Compose([\n",
    "    # A.RandomResizedCrop(256, 256, scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.GridDistortion(p=0.2),\n",
    "    A.ElasticTransform(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    A.ToTensorV2(),\n",
    "], additional_targets={'mask': 'mask'})\n",
    "\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, image_dir=img_dir, mask_dir=mask_dir, transform=transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, image_dir=img_dir, mask_dir=mask_dir,  transform=transform)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader =  DataLoader(train_dataset, batch_size=8)\n",
    "val_loader =  DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "for images, masks in train_loader:\n",
    "    print(images)\n",
    "    images, masks = images.cuda(), masks.cuda()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Check data loader looks ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def check_dataset(dataset, n=5):\n",
    "    for i in range(n):\n",
    "        sample_image, sample_mask = dataset[i]\n",
    "        #print(sample_image)\n",
    "        # print(sample_image.shape, sample_mask.shape)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(sample_image.permute(1, 2, 0))\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(sample_mask.squeeze())\n",
    "        plt.show()\n",
    "\n",
    "check_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate IoU, Dice, and other metrics.\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    # IoU\n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    union = pred_binary.sum() + target_binary.sum() - intersection\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    \n",
    "    # Dice coefficient\n",
    "    dice = (2 * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-8)\n",
    "    \n",
    "    # Pixel accuracy\n",
    "    correct = (pred_binary == target_binary).sum()\n",
    "    total = target_binary.numel()\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return {\n",
    "        'iou': iou.item(),\n",
    "        'dice': dice.item(),\n",
    "        'accuracy': accuracy.item()\n",
    "    }\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_metrics = {'iou': 0.0, 'dice': 0.0, 'accuracy': 0.0}\n",
    "    \n",
    "    for images, masks in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.shape, masks.shape, masks.unsqueeze(1).shape)\n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            batch_metrics = calculate_metrics(torch.sigmoid(outputs), masks.unsqueeze(1))\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key] += batch_metrics[key]\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    for key in train_metrics:\n",
    "        train_metrics[key] /= len(train_loader)\n",
    "    \n",
    "    return train_loss, train_metrics\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'iou': 0.0, 'dice': 0.0, 'accuracy': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1))  # Add channel dim for masks\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_metrics = calculate_metrics(torch.sigmoid(outputs), masks.unsqueeze(1))\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] += batch_metrics[key]\n",
    "    \n",
    "    # Average metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    for key in val_metrics:\n",
    "        val_metrics[key] /= len(val_loader)\n",
    "    \n",
    "    return val_loss, val_metrics\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"timm-efficientnet-b3\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    classes=1,\n",
    "    activation=None,  # Use raw logits\n",
    ")\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "    activation=None\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_iou': [], 'val_iou': [],\n",
    "    'train_dice': [], 'val_dice': [],\n",
    "    'train_accuracy': [], 'val_accuracy': []\n",
    "}\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
    "#loss_fn = smp.losses.FocalLoss(mode='binary', alpha=0.25, gamma=2, from_logits=True)\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "#loss_fn = nn.BCELoss()\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model, train_loader, loss_fn, optimizer, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_metrics = validate_model(model, val_loader, loss_fn, device) # todo: get eval loaders \n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_iou'].append(train_metrics['iou'])\n",
    "    history['val_iou'].append(val_metrics['iou'])\n",
    "    history['train_dice'].append(train_metrics['dice'])\n",
    "    history['val_dice'].append(val_metrics['dice'])\n",
    "    history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train IoU: {train_metrics['iou']:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val IoU: {val_metrics['iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # IoU\n",
    "    axes[0, 1].plot(history['train_iou'], label='Train IoU')\n",
    "    axes[0, 1].plot(history['val_iou'], label='Val IoU')\n",
    "    axes[0, 1].set_title('IoU')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Dice\n",
    "    axes[1, 0].plot(history['train_dice'], label='Train Dice')\n",
    "    axes[1, 0].plot(history['val_dice'], label='Val Dice')\n",
    "    axes[1, 0].set_title('Dice Coefficient')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1, 1].plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    axes[1, 1].plot(history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[1, 1].set_title('Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i, (images, masks) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    images = images.detach().cpu()\n",
    "    preds = outputs.detach().cpu()\n",
    "    \n",
    "    for i in range(8):\n",
    "        fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "        axs[0].imshow(images[i].permute(1, 2, 0))\n",
    "        axs[1].imshow(preds[i].squeeze() > 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    if batch_i > -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
