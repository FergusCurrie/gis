{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "- https://github.com/facebookresearch/dinov2\n",
    "- https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DINOv2/Train_a_linear_classifier_on_top_of_DINOv2_for_semantic_segmentation.ipynb#scrollTo=rLzR_mt_SnE2\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DINOv2/Fine_tune_DINOv2_for_image_classification_%5Bminimal%5D.ipynb\n",
    "- \n",
    "\n",
    "- dinov2 has 14x14 patch size. so (floor(width)x2 ) +1 hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from gis.config import Config\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_image_and_mask_files():\n",
    "    mask_files = os.listdir(config.mnt_path / 'label/18')\n",
    "    coords = []\n",
    "    for mask in mask_files:\n",
    "        x,y = mask.split('_')\n",
    "        x,y = int(x), int(y.replace('.npy', ''))\n",
    "        coords.append((x,y))\n",
    "    image_files = [f'18_{x}_{y}.jpg' for (x,y) in coords]\n",
    "    return image_files, mask_files\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks \n",
    "        self.transform = transform\n",
    "        self.image_dir = config.mnt_path / 'image/18'\n",
    "        self.mask_dir = config.mnt_path / 'label/18'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])  \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_image = np.array(image)\n",
    "        original_mask = np.load(mask_path)\n",
    "\n",
    "        transformed = self.transform(image=original_image, mask=original_mask)\n",
    "        image, target = image, target = torch.tensor(transformed['image']), torch.LongTensor(transformed['mask'])\n",
    "        image = image.permute(2,0,1)\n",
    "        return image, target, original_image, original_mask\n",
    "\n",
    "\n",
    "model_config = {\n",
    "        'batch_size': 4,\n",
    "        'epochs': 5,\n",
    "        'learning_rate': 1e-4,\n",
    "        'val_split': 0.2,\n",
    "        'num_workers': 4,\n",
    "    }\n",
    "\n",
    "\n",
    "image_files, mask_files = get_image_and_mask_files()\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    image_files, mask_files, test_size=model_config['val_split'], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "width = 256 \n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(width=width, height=width),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=width, height=width),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "\n",
    "])\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, transform=train_transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, transform=val_transform)\n",
    "\n",
    "\n",
    "\n",
    "train_loader =  DataLoader(train_dataset, batch_size=model_config['batch_size'])\n",
    "val_loader =  DataLoader(val_dataset, batch_size=model_config['batch_size'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"baclgrpimd\",\n",
    "    1: \"track\",\n",
    "}\n",
    "\n",
    "TOKEN_WIDTH = 18 # floor(image_width / 14)\n",
    "\n",
    "import torch\n",
    "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
    "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
    "\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=TOKEN_WIDTH, tokenH=TOKEN_WIDTH, num_labels=1):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.mixer = torch.nn.Conv2d(in_channels, 128, (3,3))\n",
    "        self.classifier = torch.nn.Conv2d(128, num_labels, (1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.classifier(self.mixer(embeddings))\n",
    "\n",
    "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.dinov2 = Dinov2Model(config)\n",
    "    self.classifier = LinearClassifier(config.hidden_size, TOKEN_WIDTH, TOKEN_WIDTH, 1)\n",
    "\n",
    "\n",
    "  def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
    "    # use frozen features\n",
    "    outputs = self.dinov2(pixel_values,\n",
    "                            output_hidden_states=output_hidden_states,\n",
    "                            output_attentions=output_attentions)\n",
    "    # get the patch embeddings - so we exclude the CLS token\n",
    "    # cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    patch_embeddings = outputs.last_hidden_state[:,1:,:]\n",
    "    logits = self.classifier(patch_embeddings)\n",
    "    logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "    return logits \n",
    "\n",
    "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"dinov2\"):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate IoU, Dice, and other metrics.\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    target_binary = target.float()\n",
    "    \n",
    "    # IoU\n",
    "    intersection = (pred_binary * target_binary).sum()\n",
    "    union = pred_binary.sum() + target_binary.sum() - intersection\n",
    "    iou = intersection / (union + 1e-8)\n",
    "    \n",
    "    # Dice coefficient\n",
    "    dice = (2 * intersection) / (pred_binary.sum() + target_binary.sum() + 1e-8)\n",
    "    \n",
    "    # Pixel accuracy\n",
    "    correct = (pred_binary == target_binary).sum()\n",
    "    total = target_binary.numel()\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return {\n",
    "        'iou': iou.item(),\n",
    "        'dice': dice.item(),\n",
    "        'accuracy': accuracy.item()\n",
    "    }\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_metrics = {'iou': 0.0, 'dice': 0.0, 'accuracy': 0.0}\n",
    "    \n",
    "    for images, masks, _, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        #print(outputs.shape, masks.shape, masks.unsqueeze(1).shape)\n",
    "        loss = criterion(outputs, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        with torch.no_grad():\n",
    "            batch_metrics = calculate_metrics(torch.sigmoid(outputs), masks.unsqueeze(1))\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key] += batch_metrics[key]\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    for key in train_metrics:\n",
    "        train_metrics[key] /= len(train_loader)\n",
    "    \n",
    "    return train_loss, train_metrics\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_metrics = {'iou': 0.0, 'dice': 0.0, 'accuracy': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks, _, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1))  # Add channel dim for masks\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_metrics = calculate_metrics(outputs, masks.unsqueeze(1))\n",
    "            for key in val_metrics:\n",
    "                val_metrics[key] += batch_metrics[key]\n",
    "    \n",
    "    # Average metrics\n",
    "    val_loss /= len(val_loader)\n",
    "    for key in val_metrics:\n",
    "        val_metrics[key] /= len(val_loader)\n",
    "    \n",
    "    return val_loss, val_metrics\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=model_config['learning_rate'])\n",
    "loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_iou': [], 'val_iou': [],\n",
    "    'train_dice': [], 'val_dice': [],\n",
    "    'train_accuracy': [], 'val_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss, train_metrics = train_epoch(\n",
    "        model, train_loader, loss_fn, optimizer, device\n",
    "    )\n",
    "    val_loss, val_metrics = validate_model(model, val_loader, loss_fn, device) # todo: get eval loaders \n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_iou'].append(train_metrics['iou'])\n",
    "    history['val_iou'].append(val_metrics['iou'])\n",
    "    history['train_dice'].append(train_metrics['dice'])\n",
    "    history['val_dice'].append(val_metrics['dice'])\n",
    "    history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train IoU: {train_metrics['iou']:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val IoU: {val_metrics['iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "    axes[0, 0].set_title('Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # IoU\n",
    "    axes[0, 1].plot(history['train_iou'], label='Train IoU')\n",
    "    axes[0, 1].plot(history['val_iou'], label='Val IoU')\n",
    "    axes[0, 1].set_title('IoU')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Dice\n",
    "    axes[1, 0].plot(history['train_dice'], label='Train Dice')\n",
    "    axes[1, 0].plot(history['val_dice'], label='Val Dice')\n",
    "    axes[1, 0].set_title('Dice Coefficient')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1, 1].plot(history['train_accuracy'], label='Train Accuracy')\n",
    "    axes[1, 1].plot(history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[1, 1].set_title('Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_i, (images, masks, original_images, _) in enumerate(train_loader):\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    images = images.detach().cpu()\n",
    "    masks = masks.detach().cpu()\n",
    "    preds = outputs.detach().cpu()\n",
    "    \n",
    "    for i in range(model_config['batch_size']):\n",
    "        fig, axs = plt.subplots(1,4,figsize=(20,5))\n",
    "        axs[0].imshow(images[i].permute(1, 2, 0))\n",
    "        axs[1].imshow(preds[i].squeeze() > 0)\n",
    "        axs[2].imshow(original_images[i])\n",
    "        axs[3].imshow(masks[i])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    if batch_i > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path('saved_models/')\n",
    "torch.save(model.state_dict(), save_dir / \"model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
