{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "- https://github.com/facebookresearch/dinov2\n",
    "- https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DINOv2/Train_a_linear_classifier_on_top_of_DINOv2_for_semantic_segmentation.ipynb#scrollTo=rLzR_mt_SnE2\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DINOv2/Fine_tune_DINOv2_for_image_classification_%5Bminimal%5D.ipynb\n",
    "- \n",
    "\n",
    "- dinov2 has 14x14 patch size. so (floor(width)x2 ) +1 hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     task=\"image-classification\",\n",
    "#     model=\"facebook/dinov2-small-imagenet1k-1-layer\",\n",
    "#     dtype=torch.float16,\n",
    "#     device=0\n",
    "# )\n",
    "\n",
    "# pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from transformers import TorchAoConfig, AutoImageProcessor, AutoModelForImageClassification\n",
    "#from torchao.quantization import Int4WeightOnlyConfig\n",
    "from PIL import Image\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import decode_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2 \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, image_dir, mask_dir, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks \n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        # print(self.image_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])  \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_image = np.array(image)\n",
    "        original_mask = np.load(mask_path)\n",
    "\n",
    "        transformed = self.transform(image=original_image, mask=original_mask)\n",
    "        image, target = image, target = torch.tensor(transformed['image']), torch.LongTensor(transformed['mask'])\n",
    "        image = image.permute(2,0,1)\n",
    "        return image, target, original_image, original_mask # , original_image, original_segmentation_map\n",
    "        # if self.transform:\n",
    "        #     augmented = self.transform(image=np.array(image), mask=np.array(mask))\n",
    "        #     image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "        # else:\n",
    "        #     image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        #     mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        # image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "        # mask = torch.from_numpy(mask).float()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "img_dir = '/mnt/gis/image/18/'\n",
    "mask_dir = '/mnt/gis/label/18/'\n",
    "images = os.listdir(img_dir)\n",
    "masks = os.listdir(mask_dir)\n",
    "\n",
    "\n",
    "coords = []\n",
    "for mask in masks:\n",
    "    x,y = mask.split('_')\n",
    "    x,y = int(x), int(y.replace('.npy', ''))\n",
    "    coords.append((x,y))\n",
    "\n",
    "images = [f'18_{x}_{y}.jpg' for (x,y) in coords]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 5,\n",
    "        'learning_rate': 1e-4,\n",
    "        'val_split': 0.2,\n",
    "        'num_workers': 4,\n",
    "    }\n",
    "\n",
    "\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    images, masks, test_size=config['val_split'], random_state=42\n",
    ")\n",
    "\n",
    "TARGET_SIZE = (256, 256) \n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "width = 256 \n",
    "\n",
    "train_transform = A.Compose([\n",
    "    # hadded an issue with an image being too small to crop, PadIfNeeded didn't help...\n",
    "    # if anyone knows why this is happening I'm happy to read why\n",
    "    # A.PadIfNeeded(min_height=448, min_width=448),\n",
    "    # A.RandomResizedCrop(height=448, width=448),\n",
    "    A.Resize(width=width, height=width),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=width, height=width),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "\n",
    "])\n",
    "train_dataset = SegmentationDataset(train_images, train_masks, image_dir=img_dir, mask_dir=mask_dir, transform=train_transform)#, transform=transform)\n",
    "val_dataset = SegmentationDataset(val_images, val_masks, image_dir=img_dir, mask_dir=mask_dir, transform=val_transform)#,  transform=transform)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader =  DataLoader(train_dataset, batch_size=8)\n",
    "val_loader =  DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "for images, masks, o,m in train_loader:\n",
    "    print(images)\n",
    "    images, masks = images.cuda(), masks.cuda()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"other\",\n",
    "    1: \"track\",\n",
    "}\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
    "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "\n",
    "    self.dinov2 = Dinov2Model(config)\n",
    "    #self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels)\n",
    "\n",
    "  def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
    "    # use frozen features\n",
    "    outputs = self.dinov2(pixel_values,\n",
    "                            output_hidden_states=output_hidden_states,\n",
    "                            output_attentions=output_attentions)\n",
    "    # get the patch embeddings - so we exclude the CLS token\n",
    "    patch_embeddings = outputs.last_hidden_state[:,1:,:]\n",
    "    return patch_embeddings\n",
    "\n",
    "  \n",
    "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"dinov2\"):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch_image, batch_label, batch_original_image, bm = batch\n",
    "batch_image.shape, batch_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def check_dataset(dataset, n=5):\n",
    "    for i in range(n):\n",
    "        sample_image, sample_mask, sample_oimage, _ = dataset[i]\n",
    "        #print(sample_image)\n",
    "        # print(sample_image.shape, sample_mask.shape)\n",
    "        fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "        axs[0].imshow(sample_oimage)\n",
    "        axs[1].imshow(sample_image.permute(1, 2, 0))\n",
    "        axs[2].imshow(sample_mask)\n",
    "        plt.show()\n",
    "\n",
    "check_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=batch_image, labels=batch_label)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    features = outputs[i].numpy()\n",
    "    feature_dim = 768\n",
    "\n",
    "    patches = features.reshape(-1, feature_dim)\n",
    "    print(patches.shape)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    patches_scaled = scaler.fit_transform(patches)\n",
    "\n",
    "    # Apply PCA to reduce to 3 components (RGB)\n",
    "    pca = PCA(n_components=3, random_state=42)\n",
    "    pca_result = pca.fit_transform(patches_scaled) \n",
    "    pca_result.shape\n",
    "\n",
    "    pca_min = pca_result.min(axis=0)\n",
    "    pca_max = pca_result.max(axis=0)\n",
    "    pca_normalized = (pca_result - pca_min) / (pca_max - pca_min + 1e-8)\n",
    "\n",
    "\n",
    "    pca_spatial = pca_normalized.reshape(18, 18, 3)\n",
    "    pca_spatial.shape\n",
    "\n",
    "    fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "    axs[0].imshow(batch_image[i].permute(1,2,0))\n",
    "    axs[1].imshow(pca_spatial)\n",
    "    axs[2].imshow(batch_original_image[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
